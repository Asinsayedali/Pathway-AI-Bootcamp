# 3.3. Role of Context | Word Vectors Simplified 🌐🤔

Let's dive a bit deeper into the world of word vectors and explore how context comes into play. 

<div align="center">
<img src="https://github.com/gtech-mulearn/Pathway-AI-Bootcamp/blob/main/WVS%20ChatGPT.png"height='200'>
</div>

Imagine you're trying to understand the word "apple." Without context, it could be a fruit or a tech company. 🍏💻 But what if I say, "I ate an apple"? Now it's clear, right? Context helps us make sense of words, and it's no different for large language models. 🍏🍴

**Technical Explanation Made Simple** 🤓

In general, large language models like GPT-4 or Llama use various techniques to understand the context surrounding each word. For instance, GPT-4 leverages a popular and efficient technique called the "attention mechanism," which helps the model focus on different parts of the text to understand it better. 🧠🔍 However, older models might use other strategies like Recurrent Neural Networks (RNNs) or Long Short-Term Memory Networks (LSTMs) to capture context in a different way. 📚💡
****
